{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "7d39df20_cde02a58",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1002536
      },
      "writtenOn": "2024-06-12T16:16:41Z",
      "side": 1,
      "message": "Please take a look, thanks!\n\nThis CL add a shader implementation (namely `WorkgroupShared`) that split input vector \u0026 matrix into blocks and each invocation within a workgroup compute multiplication of a block to get a partial result, and at the end add up all result using workgroup shared memory to output the final result of the workgroup. This schedule schema shows surprising performance when I try to optimize Mat-Vec-Mul kernels.\n\nThis CL also remove the suppression of subgroups implementation on D3D12 devices, since the issue chromium:42241412 seems no longer happen.\n\nData below show the testing result on Intel A770 GPU with `disable_robustness` enabled, cases with `Impl_WorkgroupShared` are added workgroup shared implementation. Such `WorkgroupShared` implementation shows comparable or better performance even over subgroups implementation.\n\n| Case | Avg GPU time (ms) |\n| - | - |\n| Rows_32768_Cols_2048_StoreType_F32_AccType_F32_Impl_Naive_Swizzle_0 | `1.2929628646666667` |\n| Rows_32768_Cols_2048_StoreType_F32_AccType_F32_Impl_Naive_Swizzle_1 | `1.5151195023333333` |\n| Rows_32768_Cols_2048_StoreType_F32_AccType_F32_Impl_Subgroup_Swizzle_0 | `1.581165226666667` |\n| Rows_32768_Cols_2048_StoreType_F32_AccType_F32_Impl_Subgroup_Swizzle_1 | `1.5630336` |\n| Rows_32768_Cols_2048_StoreType_F32_AccType_F32_Impl_WorkgroupShared_Swizzle_0 | `1.2738950096666668` |\n| Rows_32768_Cols_2048_StoreType_F32_AccType_F32_Impl_WorkgroupShared_Swizzle_1 | `0.6512093866666667` |\n| Rows_32768_Cols_2048_StoreType_F16_AccType_F16_Impl_Naive_Swizzle_0 | `0.49447486877200003` |\n| Rows_32768_Cols_2048_StoreType_F16_AccType_F16_Impl_Naive_Swizzle_1 | `0.7393805128203335` |\n| Rows_32768_Cols_2048_StoreType_F16_AccType_F16_Impl_Subgroup_Swizzle_0 | `0.6051976533333333` |\n| Rows_32768_Cols_2048_StoreType_F16_AccType_F16_Impl_Subgroup_Swizzle_1 | `0.8893433793936666` |\n| Rows_32768_Cols_2048_StoreType_F16_AccType_F16_Impl_WorkgroupShared_Swizzle_0 | `0.7717228088890001` |\n| Rows_32768_Cols_2048_StoreType_F16_AccType_F16_Impl_WorkgroupShared_Swizzle_1 | `0.349023699753` |\n| Rows_32768_Cols_2048_StoreType_U8_AccType_F32_Impl_Naive_Swizzle_0 | `0.2539431437836667` |\n| Rows_32768_Cols_2048_StoreType_U8_AccType_F32_Impl_Naive_Swizzle_1 | `0.3103639324443333` |\n| Rows_32768_Cols_2048_StoreType_U8_AccType_F32_Impl_Subgroup_Swizzle_0 | `0.32017248711100005` |\n| Rows_32768_Cols_2048_StoreType_U8_AccType_F32_Impl_Subgroup_Swizzle_1 | `0.387936711111` |\n| Rows_32768_Cols_2048_StoreType_U8_AccType_F32_Impl_WorkgroupShared_Swizzle_0 | `0.23162447089433333` |\n| Rows_32768_Cols_2048_StoreType_U8_AccType_F32_Impl_WorkgroupShared_Swizzle_1 | `0.19202958222233332` |\n| Rows_32768_Cols_2048_StoreType_U8_AccType_F16_Impl_Naive_Swizzle_0 | `0.2578113422223333` |\n| Rows_32768_Cols_2048_StoreType_U8_AccType_F16_Impl_Naive_Swizzle_1 | `0.31960450844433336` |\n| Rows_32768_Cols_2048_StoreType_U8_AccType_F16_Impl_Subgroup_Swizzle_0 | `0.333968335238` |\n| Rows_32768_Cols_2048_StoreType_U8_AccType_F16_Impl_Subgroup_Swizzle_1 | `0.419354416232` |\n| Rows_32768_Cols_2048_StoreType_U8_AccType_F16_Impl_WorkgroupShared_Swizzle_0 | `0.21963196527166665` |\n| Rows_32768_Cols_2048_StoreType_U8_AccType_F16_Impl_WorkgroupShared_Swizzle_1 | `0.18937674884366665` |\n| Rows_32768_Cols_2048_StoreType_U8_AccType_U8_Impl_Naive_Swizzle_0 | `0.25169366486500006` |\n| Rows_32768_Cols_2048_StoreType_U8_AccType_U8_Impl_Naive_Swizzle_1 | `0.3038403991396667` |\n| Rows_32768_Cols_2048_StoreType_U8_AccType_U8_Impl_Subgroup_Swizzle_0 | `0.285386110707` |\n| Rows_32768_Cols_2048_StoreType_U8_AccType_U8_Impl_Subgroup_Swizzle_1 | `0.3659765497433334` |\n| Rows_32768_Cols_2048_StoreType_U8_AccType_U8_Impl_WorkgroupShared_Swizzle_0 | `0.1850605568` |\n| Rows_32768_Cols_2048_StoreType_U8_AccType_U8_Impl_WorkgroupShared_Swizzle_1 | `0.17736310153833335` |",
      "revId": "961fd7a5269d69de7f6838678100229113720d01",
      "serverId": "dd02978d-1a8e-36d7-bcc0-a5723e5c0abd"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "31180777_39cb0c46",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1000033
      },
      "writtenOn": "2024-06-12T20:02:31Z",
      "side": 1,
      "message": "Looks good to me, and thanks!\n\nI\u0027ll leave it to Austin or Corentin to approve.\nAre there concerns about tests taking too long to run?   This increases the number of permutations by 50%",
      "revId": "961fd7a5269d69de7f6838678100229113720d01",
      "serverId": "dd02978d-1a8e-36d7-bcc0-a5723e5c0abd"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "8e80665a_75ae6675",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1000032
      },
      "writtenOn": "2024-06-13T17:14:35Z",
      "side": 1,
      "message": "It looks like we\u0027ve added a minute to the test time of dawn_perf_tests. It seems reasonable. I think we can go ahead and do something like shrink the matrix size if there is a capacity problem.",
      "parentUuid": "31180777_39cb0c46",
      "revId": "961fd7a5269d69de7f6838678100229113720d01",
      "serverId": "dd02978d-1a8e-36d7-bcc0-a5723e5c0abd"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "ae3fd1bc_f4983176",
        "filename": "src/dawn/tests/perf_tests/MatrixVectorMultiplyPerf.cpp",
        "patchSetId": 2
      },
      "lineNbr": 427,
      "author": {
        "id": 1000032
      },
      "writtenOn": "2024-06-13T17:14:35Z",
      "side": 1,
      "message": "note: this could be a parallel reduction:\n(untested, but something like)\n\n```\nfor (var stride \u003d wg_size \u003e\u003e 1; stride \u003e 0; stride \u003d stride \u003e\u003e 1) {\n  workgroupBarrier();\n  if (local_id.x \u003c i) {\n    sum \u003d workgroup_sum[i] + workgroup_sum[i + stride];\n    workgroup_sum[i] \u003d sum;\n  }\n}\nif (local_id.x \u003d\u003d 0) {\n  writeResult\n}\n```\n\nWe should of course - test that this is actually faster. I believe it should be as it allows for more parallelism, at the cost of some extra stores to shared memory.\n\nIt could be made even faster with subgroupAdd. Each subgroup can perform the subgroupAdd of the `sum` variable and write that to shared memory. and then, sum up the results in shared memory.\nTint doesn\u0027t have subgroupAdd yet though (just fyi)",
      "range": {
        "startLine": 422,
        "startChar": 2,
        "endLine": 427,
        "endChar": 3
      },
      "revId": "961fd7a5269d69de7f6838678100229113720d01",
      "serverId": "dd02978d-1a8e-36d7-bcc0-a5723e5c0abd"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "71d6220e_9b742cb6",
        "filename": "src/dawn/tests/perf_tests/MatrixVectorMultiplyPerf.cpp",
        "patchSetId": 2
      },
      "lineNbr": 427,
      "author": {
        "id": 1002536
      },
      "writtenOn": "2024-06-18T09:30:13Z",
      "side": 1,
      "message": "Done changing to parallel sum-up, but no significant perf diff seen during local testing.",
      "parentUuid": "ae3fd1bc_f4983176",
      "range": {
        "startLine": 422,
        "startChar": 2,
        "endLine": 427,
        "endChar": 3
      },
      "revId": "961fd7a5269d69de7f6838678100229113720d01",
      "serverId": "dd02978d-1a8e-36d7-bcc0-a5723e5c0abd"
    }
  ]
}