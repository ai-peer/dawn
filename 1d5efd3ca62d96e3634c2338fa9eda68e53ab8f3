{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "6c0e413c_6450bb85",
        "filename": "/COMMIT_MSG",
        "patchSetId": 21
      },
      "lineNbr": 22,
      "author": {
        "id": 1000002
      },
      "writtenOn": "2023-02-27T22:10:12Z",
      "side": 1,
      "message": "IDK that this should be an attribute in dawn.json because it would be very specific, but we can see about that later. In the past we\u0027ve had some hardcoded lists in template files, in this case there would just be DeviceTick/Release that needs special casing so that could be reasonable?",
      "revId": "1d5efd3ca62d96e3634c2338fa9eda68e53ab8f3",
      "serverId": "dd02978d-1a8e-36d7-bcc0-a5723e5c0abd"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "3cfbc5ba_9ccaca77",
        "filename": "/COMMIT_MSG",
        "patchSetId": 21
      },
      "lineNbr": 22,
      "author": {
        "id": 1000032
      },
      "writtenOn": "2023-02-28T00:33:21Z",
      "side": 1,
      "message": "originally suggested it be an attribute because of the ordering of needing to call the callbacks after the mutex is unlocked. and the idea for the mutex was that it was a global in the proc table, not accessible by the device. so, we would also need to put the callback-calling code in the proc table. that problem goes away though if we either:\n - have a backdoor to get the mutex as you mentioned in the other comment thread\n - have the mutex be a member of the device, and get it from an object\u0027s parent device\n\nIMO I prefer the extra indirection of getting the mutex from the parent device so that in situations where we don\u0027t need one, we can get a no-op mutex (like WebGPU JS)",
      "parentUuid": "6c0e413c_6450bb85",
      "revId": "1d5efd3ca62d96e3634c2338fa9eda68e53ab8f3",
      "serverId": "dd02978d-1a8e-36d7-bcc0-a5723e5c0abd"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "c2d9d6b1_9b1f0146",
        "filename": "/COMMIT_MSG",
        "patchSetId": 21
      },
      "lineNbr": 22,
      "author": {
        "id": 1003501
      },
      "writtenOn": "2023-02-28T06:28:31Z",
      "side": 1,
      "message": "IIRC The originally agreed idea for thread safe API is auto generating the mutex.lock()/unlock() code in the ProcTable. The mutex can be from anywhere (either owned by ProcTable or Device).\n\nI think who is the mutex\u0027s owner doesn\u0027t matter much. Since the mutex\u0027s locking code is generated at ProcTable level. By the time we go into APITick(), the mutex is already locked. As such Autstin\u0027s other idea for delaying callbacks was passing the CallbackSink from ProcTable and have it autogenerated too).\n- Example:\n  - NativeDeviceTick (autogen):\n    - CallbackSink sink (autogen)\n    - lock_guard(mutex) (autogen)\n    - APITick(\u0026sink)\n      - flush callbacks to sink.\n    - lock_guard::~lock_guard()\n      - mutex.unlock()\n    - CallbackSink::~CallbackSink()\n      - callbacks triggered.\n\n1. Hence, if we don\u0027t want to use CallbackSink, it\u0027s probably necessary to unlock and relock the mutex.\n2. Of course alternatively, we can have a special case for APITick() such as that telling the generator to skip autogenerating locking code for it and let us manually lock inside APITick() ourselves. Then we don\u0027t need to unlock and relock.\n\nbtw, this is not really important for this CL though. This CL doesn\u0027t include thread safe/locking code yet. It will be in the following up CL.\n\n@Austin, btw mutex\u0027s unlock and relock code should not be a potential pitfall if we wrap it in a custom *un*lock_guard class similar to std::lock_guard.\n```\nclass unlock_guard {\n  unlock_guard(mutex): mMutex(mutex) {mutex.unlock();}\n  ~unlock_guard() { mMutex.lock(); }\n};\n```",
      "parentUuid": "3cfbc5ba_9ccaca77",
      "revId": "1d5efd3ca62d96e3634c2338fa9eda68e53ab8f3",
      "serverId": "dd02978d-1a8e-36d7-bcc0-a5723e5c0abd"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "c1c58899_83e173a5",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 21
      },
      "lineNbr": 0,
      "author": {
        "id": 1000032
      },
      "writtenOn": "2023-02-27T18:59:26Z",
      "side": 1,
      "message": "high level question: I thought we were going to move callbacks to only happen on DeviceBase::APIDestroy(), DeviceBase::APITick(), DeviceBase::WillDropLastExternalRef\n\nI see queue methods also trigger callbacks. Is there a reason the queue could not enqueue them into DeviceBase\u0027s mCallbackTaskManager instead?",
      "revId": "1d5efd3ca62d96e3634c2338fa9eda68e53ab8f3",
      "serverId": "dd02978d-1a8e-36d7-bcc0-a5723e5c0abd"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "b73585de_e99affa8",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 21
      },
      "lineNbr": 0,
      "author": {
        "id": 1003501
      },
      "writtenOn": "2023-02-27T19:43:28Z",
      "side": 1,
      "message": "Those queue\u0027s methods call Device::Tick() internally. In case of those CopyTexture* functions, they invoke Queue::APISubmit() internally which indirectly calls Device::Tick().\n\nUnless we want different semantic meaning between Device::APITick() and Device::Tick() such as Device::Tick() won\u0027t ever trigger pending callbacks?\n\nOr maybe the original intention for those Submit()/CopyTexture* methods is only using Tick() to update serial and not for triggering callbacks?",
      "parentUuid": "c1c58899_83e173a5",
      "revId": "1d5efd3ca62d96e3634c2338fa9eda68e53ab8f3",
      "serverId": "dd02978d-1a8e-36d7-bcc0-a5723e5c0abd"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "004c94b1_5e43109e",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 21
      },
      "lineNbr": 0,
      "author": {
        "id": 1000032
      },
      "writtenOn": "2023-02-27T20:51:07Z",
      "side": 1,
      "message": "Got it - yea, I don\u0027t think we should be triggering callbacks inside Submit.\n\nI think we should split this work into a few CLs to make it more manageable.\nAlso, we might not need device.destroy() to trigger callbacks which simplifies things a bit. My thoughts:\n 1) changing the semantics of when callbacks get called. That means callbacks should only be called from APITick, not Tick. (in the future, we can move it to wgpuInstanceProcessEvents). This will include changing all the tests and stuff to correctly expect when the callbacks occur.\n\n 2) (new-ish idea) also try to change device.destroy() to also NOT call callbacks, but instead just tag them as \"destroyed\". This would mean storing the status in CallbackTask, but not calling the callback in methods like HandleShutDown.\n    This would change things such that calling device.Tick() after device.destroy() is valid and triggers callbacks. This is in the spirit of making it so that calling instanceProcessEvents after device.destroy() flushes the callbacks.\n\n 3) Flush callbacks in WillDropLastExternalRef?\n    We could do this, but I\u0027m not actually sure it is necessary if we do (2).\n\n 4) structure things to prevent deadlock on re-entrant usage in callbacks\n    If we move calling of callbacks to APITick(), I think we can could actually go with one of your earlier suggestions of unlocking the mutex before triggering all the callbacks - and locking it again after.\n    I don\u0027t want to do the manual unlock+relock in the existing code where callbacks are called many different places since IMO the unlock+lock is easy to manually do wrong. It\u0027s safer if callbacks only trigger in APITick.\nIt depends on where the mutex lives, if the mutex is a global in the proc table (not a device member), then we DO need CallbackSink to be passed in from the generated NativeProcs. However, if the mutex is a member of the device, then we don\u0027t need CallbackSink in the generator; we could do the callback handling all inside of APITick with the manual mutex unlock. Thinking about these two options, I think the latter is the better solution since in the medium term, we won\u0027t have a proc table mutex.",
      "parentUuid": "b73585de_e99affa8",
      "revId": "1d5efd3ca62d96e3634c2338fa9eda68e53ab8f3",
      "serverId": "dd02978d-1a8e-36d7-bcc0-a5723e5c0abd"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "fa303dce_1147e307",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 21
      },
      "lineNbr": 0,
      "author": {
        "id": 1000002
      },
      "writtenOn": "2023-02-27T22:10:12Z",
      "side": 1,
      "message": "Good catch on the difference between APITick and Tick that we should have, that would make sense. +1 on 1) and 2) for sure! 3) we probably still need to flush callbacks because we try to guarantee that callbacks will be called exactly once (so memory management is simpler to do for users of Dawn).\n\n4) In APITick we could call the callbacks last so that the only thing that needs to happens is unlocking the mutex, not re-locking. However my understanding is that the mutex is outside dawn::native and in the proc table, so we\u0027d need a dawn::native backdoor method to pass in the mutex?",
      "parentUuid": "004c94b1_5e43109e",
      "revId": "1d5efd3ca62d96e3634c2338fa9eda68e53ab8f3",
      "serverId": "dd02978d-1a8e-36d7-bcc0-a5723e5c0abd"
    }
  ]
}